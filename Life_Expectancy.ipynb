{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Life Expectancy</center> \n",
    "\n",
    "\n",
    "## ABSTRACT\n",
    "This project seeks to better understand the various factors that affect life expectancy and their relationship with each other and life expectancy itself.  The project will also use various machine learning techniques to predict various life expectancies by country and other indicators and provide the metrics needed to evaluate the predictions.\n",
    "\n",
    "Keywords: python, seaborn, numpy, plotly, pandas, matplotlib, scikit-learning, multiple linear regression, polynomial regression, decision tree, random forest, regression, logistic regression, machine learning\n",
    "\n",
    "## THIS PROJECT\n",
    "Life expectancy is also regarded as the key metric for assessing population’s health and the Global Health Observatory indicates that life expectancy globally increased from 66.8 years in 2000 to 73.4 in 2019.   Life expectancy is measured by the number of years a person is expected to live.  Improving life expectancy for individuals at birth depends on identifying and improving factors that negatively affect life expectancy while stressing factors that improve life expectancy.\n",
    "\n",
    "There are many factors that can affect a person’s life expectancy including a person’s sex, health factors, and other demographic data.\n",
    "\n",
    "This project seeks to better understand the various factors that affect life expectancy and their relationship with each other and life expectancy itself.  The project will also use various machine learning techniques to predict various life expectancy by country and other indicators and provide the metrics needed to evaluate the predictions.\n",
    "\n",
    "## ABOUT THE DATA\n",
    "The dataset being used in this project is comprised of data from all over the world from various countries aggregated by the World Health Organization (WHO). \n",
    "\n",
    "The data contains 2938 rows and 22 columns and was collected from 193 countries over a period of 15 years from 2000 to 2015.   \n",
    "\n",
    "## APPROACH\n",
    "This project will use Python and a variety of its libraries to explore and analyze the Life Expectancy dataset from the WHO. \n",
    "\n",
    "*\tData Cleaning & Exploration: Python, Pandas, NumPy, SciPy\n",
    "*\tVisualization: Seaborn, Matplotlib, Dataframe, Plotly, and Tableau\n",
    "*\tModeling: Sklearn (Mean Squared Error, r2 Score, Confusion Matrix, Accuracy Score, Classification Report, Roc Curve)\n",
    "*\tDeployment: Results will be published to the resources below:\n",
    "    -\tCode & Documents - https://github.com/schirko/Life_Expectancy-WHO_Data\n",
    "    -\tBlog - https://sschirko.edublogs.org/ \n",
    "    \n",
    "    \n",
    "## THE DATASET \n",
    "This dataset was obtained from Kaggle challenges via the World Health Organization.  It is comprised of data from around the world and includes many indicators for each country during the time frame of 2000-2015.  The data essentially represents a times series for the countries and features included. \n",
    "In general, I will be using life expectancy as the dependent variable and the other features as independent variables.\n",
    "\n",
    "The dataset'sfeatures.\n",
    "\n",
    "    <center>![about-the-dataset.png](attachment:about-the-dataset.png)</center>\n",
    "    \n",
    "\n",
    "## GETTING STARTED!!\n",
    "Alrighty then, let’s get started digging into the data.  For this project I’m using PyCharm as my IDE along with various Python libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and dataset load\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import dataframe_image as dfi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot\n",
    "from IPython.display import display, HTML\n",
    "from scipy.stats import stats\n",
    "from scipy.stats.mstats import winsorize\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, accuracy_score, \\\n",
    "    classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'charts' already exists.\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARIABLES\n",
    "# Load data into pandas data frame\n",
    "pd.set_option('display.width', 480)\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "df = pd.read_csv('Life_Expectancy_WHO_Data.csv')\n",
    "wins_df = pd.DataFrame()  # Winsorized dataframe holder\n",
    "\n",
    "wins_dict = {}  # Winsorized feature dictionary\n",
    "feature_vars = list(df.columns)[3:]  # List of numerical features\n",
    "\n",
    "# Check for directory called charts, create it if it doesn't exist\n",
    "directory = \"charts\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploritory Data Analysis\n",
    "My primary goal in this project is to understand the factors that affect the life expectancy.  So, the target variable will usually be life expectancy.  Before I can explore the data, it must first be cleaned by detecting and removing null-values and treating outliers.  Then I can move on to data exploration and analysis and begin to understand the data’s features.\n",
    "\n",
    "A few things I will need to know about the dataset and it’s features in order to get it properly cleaned up.  Each of these questions will be answered in the course of this project.\n",
    "    •\tWhat are the outliers for features?\n",
    "    •\tAre there missing values?\n",
    "    •\tWhat is the meaning of the variable?\n",
    "\n",
    "\n",
    "I used shape() to determine that the dataset has 2928 rows and 22 columns to begin with, and two of those columns (Country & Status) are categorical columns.  Country is self-explanatory while status indicates the level or development a country has – Developed or Developing.  Dtypes() used below provides us with a better view of the data and it’s messy column names.  No worries, we’ll clean them up!\n",
    "\n",
    "Head() and tail() give us a glimpse of the dataset and the features that it includes.  \n",
    "\n",
    "Lastely the results are exportedto png files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe\n",
    "print(\"shape(): \\n\", df.shape)\n",
    "print(\"\\n\")\n",
    "print(\"dtypes(): \\n\", df.dtypes)\n",
    "print(\"\\n\")\n",
    "dfi.export(pd.DataFrame(df.dtypes).style.background_gradient(), 'charts/Orig_Column_Headings.png')\n",
    "\n",
    "print(\"head(): \\n\", df.head(5))\n",
    "text_body = df.head(5)\n",
    "#view_df_to_image(text_body, 'Head(5)')\n",
    "df_styled = text_body.style.background_gradient()  # Style the text\n",
    "dfi.export(df_styled, 'charts/' + 'Head(5)' + '.png')  # Create image of text\n",
    "\n",
    "print(\"df.tail(5): \\n\", df.tail(5))\n",
    "text_body = df.tail(5)\n",
    "#view_df_to_image(text_body, 'Tail(5)')\n",
    "df_styled = text_body.style.background_gradient()  # Style the text\n",
    "dfi.export(df_styled, 'charts/' + 'Tail(5)' + '.png')  # Create image of text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Headings \n",
    "As we saw from the initial quick look of the data, the string values for the column headings are fairly messy so we need to address this first.  In the code below the spaces are replaced with underscores and shortened some of the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global df, feature_vars\n",
    "orig_column_heads = list(df.columns)\n",
    "new_column_heads = []\n",
    "for col in orig_column_heads:  # Replace spaces with underscores and \"title\" capitalize text\n",
    "    new_column_heads.append(col.strip().title().replace('  ', ' ').replace(' ', '_'))\n",
    "df.columns = new_column_heads\n",
    "\n",
    "# Rename column heads to properly reflect meaning\n",
    "df.rename(columns={'Thinness_1-19_Years': 'Thinness_10-19_Years'}, inplace=True)\n",
    "df.rename(columns={'Percentage_Expenditure': 'Pct_Expenditure'}, inplace=True)\n",
    "df.rename(columns={'Total_Expenditure': 'Ttl_Expend'}, inplace=True)\n",
    "df.rename(columns={'Bmi': 'BMI'}, inplace=True)\n",
    "df.rename(columns={'Gdp': 'GDP'}, inplace=True)\n",
    "df.rename(columns={'Hiv/Aids': 'HIV_AIDS'}, inplace=True)\n",
    "df.rename(columns={'Income_Composition_Of_Resources': 'Income_Comp_Of_Resources'}, inplace=True)\n",
    "feature_vars = list(df.columns)[3:]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new headings look much better and will make coding this project much easier and more readable!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df.dtypes))\n",
    "dfi.export(pd.DataFrame(df.dtypes).style.background_gradient(),\n",
    "           'charts/New_Column_Headings.png')  # Create image of text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the column names in order, let’s dig into missing values for the feature variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c## Missing Values\n",
    "Missing values can exist when we have nulls, but there can also be erroneous missing values when the values appear to be inexplicit.  Inexplicit nulls may require a more discretion when deciding how to deal with them as they may still represent actual data. \n",
    "\n",
    "In the section below missing values will be addressed by finding nulls and dealing with them through removing, imputing or interpolating those that are concerning. One way to look for values that stand out is to run describe() to generate descriptive statistics.  This will provide us a summary which is much quicker than having to manually look at all the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NULLS\n",
    "# Describe view stats on dataframe\n",
    "print(\"describe(): \\n\", df.describe().iloc[:, 1:])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>![before-after-column-headings.png](attachment:before-after-column-headings.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few of the items that stand out to me as not being probable are:  \n",
    "    <ul>\n",
    "    <li>Adult mortality has a 1 value for min.  </li>\n",
    "    <li>Infant Deaths has a min of 0 and max of 1800.   </li>\n",
    "    <li>BMI has min of 1. </li>\n",
    "    <li>Under-Five_Deaths has min of 0. </li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the features in question with boxplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for the potential outliers\n",
    "%matplotlib inline\n",
    "outlier_features = ['Adult_Mortality', 'Infant_Deaths', 'BMI', 'Under-Five_Deaths', 'GDP', 'Population']\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for i, col in enumerate(outlier_features, start=1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.xticks([1, 2, 3, 4, 5, 6], outlier_features)\n",
    "    df.boxplot(col, notch=True, patch_artist=True)\n",
    "\n",
    "pyplot.suptitle('Box Plot Outliers', fontsize=20, verticalalignment='top', horizontalalignment='center',\n",
    "                fontweight='bold')\n",
    "plt.savefig('charts/BoxPlot_Outliers.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def null_summary():\n",
    "na_values = df.isna().sum().reset_index()  # Find NA values\n",
    "na_values.columns = [\"Features\", \"Missing_Values\"]\n",
    "na_values[\"Missing_Percent\"] = round(na_values.Missing_Values / len(df) * 100, 2)\n",
    "print(\"Summarize NULLS: \\n\", na_values[na_values.Missing_Values > 0])\n",
    "\n",
    "df_nulls = na_values[na_values.Missing_Values > 0]\n",
    "dfi.export(pd.DataFrame(df_nulls).style.background_gradient(), 'charts/Summarize_Nulls.png')  # Print to image\n",
    "df.info()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears from the plots that there are some outliers with Adult Mortality and BMI which we can deal with later, but there also seem to be errors resulting in nulls with infant deaths and Under Five Deaths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inexplicit Nulls\n",
    "Let’s deal with the outliers in BMI and Adult Mortality later with explicit nulls and address the nulls with Infant Deaths and Under Five Deaths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting implicit NANs\n",
    "global df\n",
    "mort_5_percentile = np.percentile(df.Adult_Mortality.dropna(), 5)\n",
    "#df.adult_mortality = df.apply(lambda x: np.nan if x.Adult_Mortality < mort_5_percentile else x.Adult_Mortality,\n",
    "#                              axis=1)\n",
    "df.Infant_Deaths = df.Infant_Deaths.replace(0, np.nan)\n",
    "df['Under-Five_Deaths'] = df['Under-Five_Deaths'].replace(0, np.nan)  # Update global dataframe var\n",
    "df.BMI = df.apply(lambda x: np.nan if (x.BMI < 10 or x.BMI > 50) else x.BMI, axis=1)\n",
    "print(\"df.info():\", df.info())  # Look at nulls that are left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Summary\n",
    "Info() below will help take a look at the results.  At this point, the remaining missing values in the data should be attributable to explicit nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null_summary()  # Check for and summarize missing values\n",
    "# Check for and summarize missing values\n",
    "# Does basically the same as \"null_details(df=df):\", but prints to image\n",
    "na_values = df.isna().sum().reset_index()  # Find NA values\n",
    "na_values.columns = [\"Features\", \"Missing_Values\"]\n",
    "na_values[\"Missing_Percent\"] = round(na_values.Missing_Values / len(df) * 100, 2)\n",
    "print(\"Summarize NULLS: \\n\", na_values[na_values.Missing_Values > 0])\n",
    "\n",
    "df_nulls = na_values[na_values.Missing_Values > 0]\n",
    "dfi.export(pd.DataFrame(df_nulls).style.background_gradient(), 'charts/Summarize_Nulls.png')  # Print to image\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicit Nulls\n",
    "The data still has a significant amount null values so it will be necessary to look at the features in more detail to better understand where they are and their significance.  So, let’s detail the nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null_explicit_details()  # Detail the explicit nulls\n",
    "# Detail the explicit nulls\n",
    "#null_explicit_details()  # View null breakdown again\n",
    "global df\n",
    "df_cols = list(df.columns)\n",
    "cols_total_count = len(list(df.columns))\n",
    "cols_count = 0\n",
    "print(\"\\n\")\n",
    "for loc, col in enumerate(df_cols):  # Iterate through the columns to find nulls\n",
    "    null_count = df[col].isnull().sum()\n",
    "    total_count = df[col].isnull().count()\n",
    "    percent_null = round(null_count / total_count * 100, 2)\n",
    "    if null_count > 0:\n",
    "        cols_count += 1\n",
    "        print('[iloc = {}] {} has {} null values: {}% null'.format(loc, col, null_count, percent_null))\n",
    "cols_percent_null = round(cols_count / cols_total_count * 100, 2)\n",
    "print('{} out of {} columns contain null values; {}% columns contain null values.'.format(cols_count,\n",
    "                                                                                          cols_total_count,\n",
    "                                                                                          cols_percent_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the details above we can see that there are still 14 columns that contain nulls, that’s 63.64%!  The details show us that we still have big contributing factors towards the remaining null values in the features BMI, Population, GDP, and Infant Deaths.  We’ll deal with those as explicit nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEALING WITH THE EXPLICIT NULLS\n",
    "Because I am specifically dealing with life expectancy, there is no need to include data for any country that is missing the life expectancy values.  Those countries will be removed.\n",
    "Countries with no life expectancy data:\n",
    "•\tCook Islands, Dominica, Marshall Islands, Monaco, Nauru, Niue, Palau, Saint Kitts and Nevis, San Marino, Tuvalu\n",
    "\n",
    "It also looks like the BMI feature has half null values, so I do not see the value in keeping this feature.  It will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_nulls():    \n",
    "global df, feature_vars\n",
    "df.Life_Expectancy.notnull()  # Removes countries with no life expectancy data\n",
    "df.drop(columns='BMI', inplace=True)  # Most values are null for BMI so it's removed\n",
    "feature_vars = list(df.columns)[3:]  # Update global features list var    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is a times series which can be regarded as a sequence of values, imputation can be used to estimate new values for the missing values some of these features contain, specifically the big null value offenders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def imputed_data():\n",
    "global df\n",
    "df_cols = list(df.columns)\n",
    "cols_total_count = len(list(df.columns))\n",
    "cols_count = 0\n",
    "print(\"\\n\")\n",
    "for loc, col in enumerate(df_cols):  # Iterate through the columns to find nulls\n",
    "    null_count = df[col].isnull().sum()\n",
    "    total_count = df[col].isnull().count()\n",
    "    percent_null = round(null_count / total_count * 100, 2)\n",
    "    if null_count > 0:\n",
    "        cols_count += 1\n",
    "        print('[iloc = {}] {} has {} null values: {}% null'.format(loc, col, null_count, percent_null))\n",
    "cols_percent_null = round(cols_count / cols_total_count * 100, 2)\n",
    "print('{} out of {} columns contain null values; {}% columns contain null values.'.format(cols_count,\n",
    "                                                                                          cols_total_count,\n",
    "                                                                                          cols_percent_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take another look at the null details, we see that we are null free.  Join me in a sigh of relief.  Kind of feels like cleaning out a closet, doesn’t it?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def null_explicit_details():\n",
    "global df\n",
    "df_cols = list(df.columns)\n",
    "cols_total_count = len(list(df.columns))\n",
    "cols_count = 0\n",
    "print(\"\\n\")\n",
    "for loc, col in enumerate(df_cols):  # Iterate through the columns to find nulls\n",
    "    null_count = df[col].isnull().sum()\n",
    "    total_count = df[col].isnull().count()\n",
    "    percent_null = round(null_count / total_count * 100, 2)\n",
    "    if null_count > 0:\n",
    "        cols_count += 1\n",
    "        print('[iloc = {}] {} has {} null values: {}% null'.format(loc, col, null_count, percent_null))\n",
    "cols_percent_null = round(cols_count / cols_total_count * 100, 2)\n",
    "print('{} out of {} columns contain null values; {}% columns contain null values.'.format(cols_count,\n",
    "                                                                                          cols_total_count,\n",
    "                                                                                          cols_percent_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve cleaned up the column heads, missing values, dealt with implicit and explicit nulls - outliers are next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTLIERS\n",
    "In this section I will look into outliers by detecting them, visualizing them, and Winsorizing them.  Wins-a-what?  Winsoring the data will help minimize the influence of the outliers in the data and we know from the boxplots that at very least the adult mortality and BMI features have outliers. \n",
    "Let’s go outlier hunting for outliers! \n",
    "\n",
    "#### OUTLIER DETECTION\n",
    "I think the best first step here is to expand on the original box plots done for BMI and Adult Mortality by plotting histograms and boxplots for each feature.  \n",
    "\n",
    "The box plots below are one big image making it a bit lengthy, but I think its value makes it worth it the look.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers with boxplots and histograms\n",
    "plt.figure(figsize=(15, 30))\n",
    "i = 0\n",
    "for col in feature_vars:\n",
    "    i += 1\n",
    "    plt.subplot(9, 4, i)\n",
    "    plt.boxplot(df[col])\n",
    "    plt.title('{}'.format(col), fontsize=9)\n",
    "    i += 1\n",
    "    plt.subplot(9, 4, i)\n",
    "    plt.hist(df[col])\n",
    "    plt.title('{}'.format(col), fontsize=9)\n",
    "pyplot.suptitle('Detect Outliers', fontsize=16, verticalalignment='top', horizontalalignment='center',\n",
    "                fontweight='bold')\n",
    "plt.savefig('charts/Detect_Outlier_Plots.png', dpi=None, facecolor='w', edgecolor='g', orientation='portrait',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.0, metadata=None)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s a lot to unpack!  We can see that outliers are not uncommon and that’s a problem.  The biggest concern to me is that life expectancy has an outlier issue, especially given that this is our target feature.\n",
    "\n",
    "But let’s stay focused.  \n",
    "\n",
    "I would like to know the outlier details by feature so I will start there and apply Tukey’s rule which is a statistical method for identify what constitutes an outlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#outlier_summary()  # Count outliers and plot\n",
    "with open('charts/Outlier_Summary.txt', mode='w', newline='') as outlier_file:\n",
    "    outlier_writer = csv.writer(outlier_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for col in feature_vars:\n",
    "        q75, q25 = np.percentile(df[col], [75, 25])\n",
    "        iqr = q75 - q25\n",
    "        min_val = q25 - (iqr * 1.5)\n",
    "        max_val = q75 + (iqr * 1.5)\n",
    "        count = len(np.where((df[col] > max_val) | (df[col] < min_val))[0])\n",
    "        percent = round(count / len(df[col]) * 100, 2)\n",
    "\n",
    "        total_chars = 45\n",
    "        len_first_chars = 7\n",
    "        len_col = len(col)\n",
    "        chars_needed = total_chars - len_col - len_first_chars\n",
    "\n",
    "        print(len_first_chars * '-' + col + ' Outliers' + chars_needed * '-')\n",
    "        print('Count: {}'.format(count))\n",
    "        print('Percentage of Data: {}%'.format(percent))\n",
    "\n",
    "        outlier_writer.writerow([len_first_chars * '-' + col + ' Outliers' + chars_needed * '-'])\n",
    "        outlier_writer.writerow(['Count: {}'.format(count)])\n",
    "        outlier_writer.writerow(['Percentage of Data: {}%'.format(percent)])\n",
    "        outlier_writer.writerow(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier summary shows a decent number of outliers that I will need to address.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WINSORIZATION\n",
    "Let's work with the outliers.\n",
    "\n",
    "Because there are a number of features with outlier issues and each one has its own unique issues, the method I will use to address outliners is to Winsorize the data.  This method will help to minimize that influence that outliers have on each variable.  Winsorizing the data will enable me to set upper and lower limits on each feature.  This limits the outlier’s severity and influence on the data.  \n",
    "\n",
    "The parameters for winsorizing will be stored in a dictionary for easy access later in the project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set windsorization vars with limits\n",
    "def check_wins_plot(col, lower_limit=0, upper_limit=0, show_plot=True):\n",
    "    global wins_dict\n",
    "    wins_data = winsorize(df[col], limits=(lower_limit, upper_limit))  # Create winsorized col entries\n",
    "    wins_dict[col] = wins_data  # Add the winsorized data to the wins dictionary\n",
    "\n",
    "    if show_plot == True:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.boxplot(df[col])\n",
    "        plt.title('Original Data {}'.format(col), fontweight='bold')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.boxplot(wins_data)\n",
    "        pyplot.suptitle('Compare Before/After Winsorized\\n', fontsize=16, verticalalignment='top',\n",
    "                        horizontalalignment='center', fontweight='bold')\n",
    "        plt.title('Winsorised Data=({},{}) {}'.format(0, 0, col), fontweight='bold')\n",
    "        plt.savefig('charts/Compare_Winsorized' + col + '.png', dpi=None, facecolor='w', edgecolor='g',\n",
    "                    orientation='landscape',\n",
    "                    format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "        plt.show()\n",
    "        \n",
    "check_wins_plot(feature_vars[0], lower_limit=.01, show_plot=True)\n",
    "check_wins_plot(feature_vars[1], upper_limit=.04, show_plot=False)\n",
    "check_wins_plot(feature_vars[2], upper_limit=.05, show_plot=False)\n",
    "check_wins_plot(feature_vars[3], upper_limit=.0025, show_plot=False)\n",
    "check_wins_plot(feature_vars[4], upper_limit=.135, show_plot=False)\n",
    "check_wins_plot(feature_vars[5], lower_limit=.1, show_plot=False)\n",
    "check_wins_plot(feature_vars[6], upper_limit=.19, show_plot=False)\n",
    "check_wins_plot(feature_vars[7], upper_limit=.05, show_plot=False)\n",
    "check_wins_plot(feature_vars[8], lower_limit=.1, show_plot=False)\n",
    "check_wins_plot(feature_vars[9], upper_limit=.02, show_plot=False)\n",
    "check_wins_plot(feature_vars[10], lower_limit=.105, show_plot=False)\n",
    "check_wins_plot(feature_vars[11], upper_limit=.185, show_plot=True)\n",
    "check_wins_plot(feature_vars[12], upper_limit=.105, show_plot=False)\n",
    "check_wins_plot(feature_vars[13], upper_limit=.07, show_plot=False)\n",
    "check_wins_plot(feature_vars[14], upper_limit=.035, show_plot=False)\n",
    "check_wins_plot(feature_vars[15], upper_limit=.035, show_plot=False)\n",
    "check_wins_plot(feature_vars[16], lower_limit=.05, show_plot=False)\n",
    "check_wins_plot(feature_vars[17], lower_limit=.025, upper_limit=.005, show_plot=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplots above are illustrations of successful winsorization for life expectancy and GDP.  These two are just shown as examples of the winsorization results for all numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are into the exploration phase, a new dataframe will be created from the winsorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_wins_df()  # Create winsorized columns\n",
    "global wins_df\n",
    "wins_df = df.iloc[:, 0:3]  # Creating new winsorized dataframe\n",
    "for col in list(df.columns)[3:]:\n",
    "    wins_df[col] = wins_dict[col]  # Replace dict data with winsorized data\n",
    "\n",
    "print(\"wins_df.describe(): \\n\", wins_df.describe())  # Show descriptive statistics\n",
    "print(\"wins_df.describe(include='O'): \\n\", wins_df.describe(include='O'))  # Show count/frequency/max/quartiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIVARIATE ANALYSIS\n",
    "A univariate analysis is about looking into variables one at a time.  To do this I will first use describe() to show descriptive statistics, histograms for our continuous features and barplots for the categorical features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if the winsorization has had a big impact on many features, and not so much with other features.  Overall, the data looks much tighter for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Distributions\n",
    "Next, histograms are used for our continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def visualize_wins_dist():\n",
    "plt.figure(figsize=(15, 20))\n",
    "for i, col in enumerate(feature_vars, 1):\n",
    "    plt.subplot(5, 4, i)\n",
    "    plt.hist(wins_dict[col])\n",
    "    plt.title(col)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "plt.savefig('charts/Visualize_Winsorized.png', dpi=None, facecolor='w', edgecolor='g', orientation='portrait',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears as if the winsorization has had a big impact on many features, and not so much with other features.  Overall, the data looks much tighter for each feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that winsorization was consistently applied\n",
    "#def check_wins_country():\n",
    "plt.figure(figsize=(15, 25))\n",
    "wins_df.Country.value_counts(ascending=True).plot(kind='barh')\n",
    "plt.title('Check of Winsorization by Country')\n",
    "plt.xlabel('Count of Rows')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.savefig('charts/Check_Wins_by_Country.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above is not visually stimulating; however, it shows that all of the remaining countries have 16 rows worth of data. Good to see that all the countries are being represented in the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that winsorization was been applied correctly by Year\n",
    "#def check_wins_year():\n",
    "wins_df.Year.value_counts().sort_index().plot(kind='barh')\n",
    "plt.title('Check of Winsorization by Year')\n",
    "plt.xlabel('Count of Rows')\n",
    "plt.ylabel('Year')\n",
    "plt.savefig('charts/Check_Wins_by_Year.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is another plot that doesn’t dazzle the eyes but show us that each year has the same number of rows of data. Another score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the volume of data from the standpoint of a developing or developed country status.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Life Expectancy grouped by country and status\n",
    "#def le_by_status():\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "wins_df.Status.value_counts().plot(kind='bar')\n",
    "plt.title('Life Expectancy by Country Status', fontweight='bold')\n",
    "plt.xlabel('Country Status')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.xticks(rotation=0)\n",
    "plt.subplot(1, 2, 2)\n",
    "wins_df.Status.value_counts().plot(kind='pie', autopct='%.2f')\n",
    "plt.ylabel('')\n",
    "plt.title('Country Status Pie Chart', fontweight='bold')\n",
    "plt.savefig('charts/Life_Expectancy_by_Country_Status.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s significant about these next two charts is the amount of data that comes from the developing countries versus developed countries.  The pie chart shows that 82.51% of all the data comes from countries which have the developing Status!   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Analysis\n",
    "In this section I look at the continuous features compared to each other as well as their relationship to the life expectancy target feature. \n",
    "  \n",
    "### Multi-Collinearity\n",
    "The two heatmaps below are quite beneficial as they layout the correlations in a manner that is very easy to read.  Same data is used in both heatmaps, just a different way to illustrate the correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of winsorized feature's correlation\n",
    "#corr_matrix_heatmap()  \n",
    "mask = np.triu(wins_df[feature_vars].corr())  # Get upper triangle of array\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplots_adjust(bottom=.2)\n",
    "sns.heatmap(wins_df[feature_vars].corr(), annot=True, fmt='.2g', vmin=-1, vmax=1, center=0,\n",
    "            cmap='Spectral', mask=mask)\n",
    "plt.ylim(18, 0)\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=24, verticalalignment='top', horizontalalignment='center',\n",
    "          color='black', fontweight='bold')\n",
    "plt.savefig('charts/Corr_Matrix_Heatmap.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.25, metadata=None)\n",
    "plt.show()\n",
    "\n",
    "#def create_data_corr():\n",
    "df_styled = pd.DataFrame(wins_df.corr()).style.background_gradient()  # Style the text\n",
    "dfi.export(df_styled, 'charts/Create_Data_Corr.png')  # Create image from text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of observations can be made including the somewhat high correlation between life expectancy, the target variable, and Adult Mortality, HIV/AIDS, income of composition resources, and schooling.  \n",
    "\n",
    "Here are a few other observations:\n",
    "<ul>\n",
    "<li>Infant deaths and Under Five deaths are extremely highly correlated</li>\n",
    "<li>GDP and Percentage Expenditure are fairly highly correlated</li>\n",
    "<li>Diphtheria and Polio vaccine rate are highly positively correlated</li>\n",
    "<li>Hepatitis B vaccine rate is positively correlated with Polio </li>\n",
    "<li>Hepatitis B vaccine rate is positively correlated with and Diphtheria vaccine rates</li>\n",
    "<li>Life Expectancy has almost no correlation to Population.</li>\n",
    "<li>Schooling and Income Composition of Resources are highly correlated</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional perspectives on correlation the chart below contains a matrix of scatter plots showing correlation among the numerical features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pairs for correlation testing\n",
    "#def pairplot_corr():\n",
    "sns.pairplot(wins_df,\n",
    "             vars=['Adult_Mortality', 'Ttl_Expend', 'GDP', 'Income_Comp_Of_Resources', 'Schooling'],\n",
    "             hue='Status')\n",
    "plt.savefig('charts/Pairplot_Correlation.png', dpi=None, facecolor='w', edgecolor='g',\n",
    "            orientation='landscape', format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATEGORICAL DATA\n",
    "How has life expectancy changed over the years?  Let’s look at a line plot showing a time series of life expectancy from 2000 to 2015.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Life Expectancy time series by Year\n",
    "sns.lineplot(data=wins_df, x='Year', y='Life_Expectancy', marker='^')\n",
    "plt.title('Life Expectancy by Year', fontweight='bold')\n",
    "plt.savefig('charts/Line_Plot_Life_Expectancy_by_Year.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "wins_df.Year.corr(wins_df.Life_Expectancy)  # Find the pairwise correlation\n",
    "\n",
    "# Time series line plot of Life Expectancy by status\n",
    "#def country_status_lineplot():\n",
    "le_year = wins_df.groupby(by=['Year', 'Status']).mean().reset_index()\n",
    "Developed = le_year.loc[le_year['Status'] == 'Developed', :]\n",
    "Developing = le_year.loc[le_year['Status'] == 'Developing', :]\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(x=Developing['Year'], y=Developing['Life_Expectancy'],\n",
    "                          mode='lines',\n",
    "                          name='Developing',\n",
    "                          marker_color='blue'))\n",
    "fig1.add_trace(go.Scatter(x=Developed['Year'], y=Developed['Life_Expectancy'],\n",
    "                          mode='lines',\n",
    "                          name='Developed',\n",
    "                          marker_color='orange'))\n",
    "fig1.update_layout(\n",
    "    height=500,\n",
    "    xaxis_title=\"Years\",\n",
    "    yaxis_title='Life expectancy in age',\n",
    "    title_text='<b>Average Life Expectancy by Country Status</b>')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There’s definitely a positive trend with life expectancy over the 15 years of data provided.  The next chart below are reminder that the majority of our data comes from developing countries, yet the lowest life expectancy comes from the same developing countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot by country status\n",
    "#def boxplot_status():\n",
    "sns.boxplot(x=wins_df['Status'], y=wins_df['Life_Expectancy'], fliersize=5)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.ylabel(\"Life Expectancy\", fontsize=12)\n",
    "plt.xlabel(\"Status\", fontsize=12)\n",
    "plt.title(\"Life Expectancy by Country Status\", fontweight='bold', fontsize=13)\n",
    "plt.savefig('charts/Boxplot_Status.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional information to think about I’ve done a density plot of life expectancy which shows that highest density in the lower to mid-70s.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Life Expectancy density\n",
    "#def le_desity_histplot():\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title('Density - Life_Expectancy', fontweight='bold')\n",
    "sns.histplot(wins_df['Life_Expectancy'].dropna(), kde=True, color=\"darkgoldenrod\")\n",
    "plt.savefig('charts/Density - Life_Expectancy.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know where the correlation exist, how about any geographical correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional information to think about I’ve done a density plot of life expectancy which shows that highest density in the lower to mid-70s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing features with high correlation\n",
    "#remove_features()  \n",
    "global wins_df, feature_vars\n",
    "rem_features = ['Thinness_5-9_Years', 'GDP', 'Infant_Deaths', 'Population']\n",
    "for f in rem_features:\n",
    "    wins_df.drop(f, axis=1)\n",
    "feature_vars = list(wins_df.columns)[3:]\n",
    "\n",
    "print(\"\\n Remove Features Confirmation: \\n\", wins_df.info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Life Expectancy Ranking Summaries\n",
    "We can also take a look at the “top” and “bottom” lists by mean and median to visually inspect the countries that stand out at the top and bottom.  As expected, based on the information above has indicated, the “bottom” lists are filled with African countries.  While This project does not do a deep dive into the features that represent root causes of lower or higher life expectancy, future projects might use this project as a spring board to do that deep dive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def le_rankings(rank, stat_type, num_rows):\n",
    "    order = False if rank == 'Top' else True\n",
    "\n",
    "    # Generate text body for rankings\n",
    "    if stat_type == 'Mean':\n",
    "        text_body = pd.DataFrame(\n",
    "            wins_df.groupby(['Country']).Life_Expectancy.mean().sort_values(ascending=order).head(num_rows))\n",
    "    elif stat_type == 'Median':\n",
    "        text_body = pd.DataFrame(\n",
    "            wins_df.groupby(['Country']).Life_Expectancy.median().sort_values(ascending=order).head(num_rows))\n",
    "\n",
    "    # Create images from text for all Top, Bottom, Mean, and Median combinations\n",
    "    title = rank + ' ' + str(num_rows) + ' Life Expectancy (' + stat_type + ')'\n",
    "    df_styled = text_body.style.background_gradient()  # Style the image\n",
    "    dfi.export(df_styled, 'charts/' + rank + '_' + stat_type + '_' + 'Rankings.png')  # Create image from text\n",
    "    print(\"\\n\" + str(title) + \"\\n\" + str(text_body))\n",
    "      \n",
    "    \n",
    "le_rankings('Top', 'Mean', 15)  # For Top Mean Countries\n",
    "le_rankings('Top', 'Median', 15)  # For Top Median Countries\n",
    "le_rankings('Bottom', 'Mean', 15)  # For Bottom Mean Countries\n",
    "le_rankings('Bottom', 'Median', 15)  # For Bottom Median Countries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if there is a statical difference between developed and developing I’ll run a t-test to test the hypothesis that there is a significant statistical difference.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the t-test shows a p-value of 1.478 indicating that there is a significant difference between developing and develop countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Life Expectancy Features\n",
    "I wanted to analyze two of the independent numerical two features to get an idea of their density and correlation to Life Expectancy and decided to do this with Schooling and Income of Resources.  IoR represents how productive resources are used in a country and Schooling represents number of years in school.  Both have shown to be correlated to Life Expectancy so let’s look a bit more at them.\n",
    "\n",
    "The density plots represent all countries regardless of Status, but we’ll use that as the basis for our conclusions.    It would certainly be useful to break them down with respect to Status in my next project.  The scatter plots are crystal clear in illustrating the correlation between Schooling and Life Expectancy and Income of Resources and Life Expectancy.  WOW!  It almost seems like a miracle to live past 80 years old in a developing country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Life Expectancy density\n",
    "#def le_desity_histplot():\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title('Density - Income_Comp_Of_Resources', fontweight='bold')\n",
    "sns.histplot(wins_df['Income_Comp_Of_Resources'].dropna(), kde=True, color=\"darkgoldenrod\")\n",
    "plt.savefig('charts/Density - Income_Comp_Of_Resources.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title('Density - Schooling', fontweight='bold')\n",
    "sns.histplot(wins_df['Schooling'].dropna(), kde=True, color=\"darkgoldenrod\")\n",
    "plt.savefig('charts/Density - Schooling.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the use of scatterplots to show correlation between develop/developing countries and other variables. The charts I found particularly intesting where the ones showing alchohol, hepatitis-b, and the measeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot showing feature correlation\n",
    "#def scatterplot_features_corr():\n",
    "wins_df_copy = wins_df.drop(['Life_Expectancy'], axis=1)\n",
    "categorical_cols = wins_df_copy.select_dtypes(include=\"O\")\n",
    "numerical_cols = wins_df_copy.select_dtypes(exclude=\"O\")\n",
    "\n",
    "for col in numerical_cols.columns:\n",
    "    sns.scatterplot(x=numerical_cols[col], y=wins_df[\"Life_Expectancy\"], hue=categorical_cols.Status)\n",
    "    plt.xticks(rotation=90, fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.ylabel(\"Life Expectancy\", fontsize=10)\n",
    "    plt.xlabel(col, fontsize=10)\n",
    "    plt.title(col + ' to Life Expectancy Correlation', fontweight='bold', fontsize=12)\n",
    "    plt.savefig('charts/Scatterplot_' + col + '_corr.png', dpi=None, facecolor='w', edgecolor='g',\n",
    "                orientation='landscape',\n",
    "                format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN, TEST, and EVALUATE MODEL (MACHINE LEARNING)\n",
    "So here we are, ready to model and predict!  In this section I will walk through linear regression, multiple regression, polynomial regression, decision tree, random forest, and logistical regression.  \n",
    "\n",
    "### Linear Regression\n",
    "For our modeling and predicting, let’s start off with linear regression to model the relationships between our independent and dependent variables.   I’ll do this with the GDP and Income Composition of Resources. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression on GDP and Percentage Expenditure\n",
    "#def linear_regression():\n",
    "lr_df = wins_df.dropna().copy()  # Copy winsorized dataframe\n",
    "regr = LinearRegression()\n",
    "x = lr_df.GDP.values.reshape(-1, 1)\n",
    "y = lr_df['Pct_Expenditure'].values.reshape(-1, 1)\n",
    "regr.fit(x, y)  # Fit the linear model\n",
    "\n",
    "lr_predict = regr.predict(([[10000]]))\n",
    "print(\"regr.predict: \", lr_predict)\n",
    "lr_coef = regr.coef_\n",
    "print(\"lr_coef: \", lr_coef)\n",
    "\n",
    "x_array = np.arange(min(lr_df.GDP), max(lr_df.GDP)).reshape(-1, 1)  # Create prediction line\n",
    "plt.scatter(x, y)  # Plot the regression\n",
    "y_head = regr.predict(x_array)  # Predict percentage of expenditure\n",
    "\n",
    "plt.plot(x_array, y_head, color=\"red\")\n",
    "plt.ylabel(\"Percentage Expenditure\", fontsize=12)\n",
    "plt.xlabel(\"GDP\", fontsize=12)\n",
    "plt.title(\"Linear Regression - GDP & Percentage Expenditure\", fontweight='bold', fontsize=13)\n",
    "plt.savefig('charts/Linear_Regression.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()\n",
    "\n",
    "print(\"r2 Score: \", r2_score(y, regr.predict(x)))\n",
    "print(\"Mean Absolute Error: {:.4f}\".format(metrics.mean_absolute_error(x_array, y_head)))\n",
    "print(\"Mean Squared Error: {:.4f}\".format(metrics.mean_squared_error(x_array, y_head)))\n",
    "print(\"Root Mean Squared Error: {:.4f}\".format(np.sqrt(metrics.mean_squared_error(x_array, y_head))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above fits the linear regression model, calculates a few statistics, and plots the regression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "With multiple linear regression I would like to look at Life Expectancy, the independent variable.  \n",
    "\n",
    "Results from multiple linear regression prediction are below.  The correctness of the model is good.  It’s the difference between the training error and testing error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple linear regression of winsorized data\n",
    "#def multiple_linear_regression():\n",
    "mlr_df = wins_df.dropna().copy()  # Copy winsorized dataframe\n",
    "\n",
    "mlr_df.drop([\"Country\", \"Status\"], axis=1, inplace=True)  # Drop because the are categorical\n",
    "x = mlr_df.iloc[:, [-2, -1]].values  # The dependent variables/features\n",
    "print(\"print X: \", x)\n",
    "y = mlr_df['Pct_Expenditure'].values.reshape(-1, 1)  # The independent variable/features\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)  # split data\n",
    "regr = LinearRegression()\n",
    "model = regr.fit(x_train, y_train)  # Fit the LR model\n",
    "\n",
    "print(\"b0: \", regr.intercept_)\n",
    "print(\"b1,b2: \", regr.coef_)\n",
    "\n",
    "new_data = [[0.4, 8], [0.5, 10]]  # Random data to test\n",
    "new_data = pd.DataFrame(new_data).T\n",
    "model.predict(new_data)\n",
    "\n",
    "# Mean squared error regression loss\n",
    "mserl = np.sqrt(mean_squared_error(y_train, model.predict(x_train)))\n",
    "print(\"Mean squared error regression loss: \", mserl)\n",
    "\n",
    "# Train and predict model\n",
    "model.score(x_train, y_train)  # Coefficient of determination\n",
    "cross_val_score(model, x_train, y_train, cv=10, scoring=\"r2\").mean()  # Evaluate the score by cross-validation\n",
    "y_head = model.predict(x_test)\n",
    "y_head[0:5]  # Get first 5\n",
    "\n",
    "# Calculate r2 score\n",
    "r2_degeri = r2_score(y_test, y_head)\n",
    "print(\"\\nTest r2 Error = \", r2_degeri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "Polynomial regression can be beneficial in some cases where the data may have a a curvilinear relationship between the target variable and the independent variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression of winsorized data\n",
    "#def polynomial_regression():\n",
    "poly_df = wins_df.dropna().copy()  # Copy winsorized dataframe\n",
    "regr = LinearRegression()\n",
    "x = poly_df.GDP.values.reshape(-1, 1)\n",
    "y = poly_df['Pct_Expenditure'].values.reshape(-1, 1)\n",
    "regr.fit(x, y)  # Fit linear model first\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)  # Split data\n",
    "\n",
    "poly_regr = PolynomialFeatures(degree=15)\n",
    "x_polynomial = poly_regr.fit_transform(x)\n",
    "regr.fit(x_polynomial, y)  # Fit the polynomial features model\n",
    "y_head = regr.predict(x_polynomial)\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=8)\n",
    "level_poly = poly_features.fit_transform(x_train)\n",
    "regr.fit(level_poly, y_train)  # Fit the trained model\n",
    "y_head = regr.predict(poly_features.fit_transform(x_train))\n",
    "y_test = np.array(range(0, len(y_train)))\n",
    "\n",
    "r2 = r2_score(y_train, y_head)\n",
    "print(\"r2 Value: \", r2)  # percentage of significance\n",
    "\n",
    "plt.scatter(y_test, y_train, color=\"blue\")\n",
    "plt.scatter(y_test, y_head, color=\"orange\")\n",
    "plt.xlabel(\"GDP\")\n",
    "plt.ylabel(\"Percentage Expenditure\")\n",
    "plt.title(\"Polynomial Regression - Percentage Expenditure\", fontweight='bold', fontsize=13)\n",
    "plt.savefig('charts/Polynomial_Regression.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from the polynomial regression prediction show a r2 value of 0.22218851598763079"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression\n",
    "This function is a look at Percent Expenditure estimation of a Country with \"GDP\" value of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression of winsorized data\n",
    "#def decision_tree_regression():\n",
    "dtr_df = wins_df.dropna().copy()  # Copy winsorized dataframe\n",
    "\n",
    "x = dtr_df.GDP.values.reshape(-1, 1)\n",
    "y = dtr_df['Pct_Expenditure'].values.reshape(-1, 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)  # split data\n",
    "\n",
    "regr = DecisionTreeRegressor()  # created model\n",
    "regr.fit(x_train, y_train)  # fitted model according to train values\n",
    "\n",
    "print(\"Decision Tree Prediction: \", regr.predict([[1000]]))\n",
    "\n",
    "x_array = np.arange(min(x), max(x), 0.01).reshape(-1, 1)  # line information to be drawn as a predict\n",
    "y_head = regr.predict(x_array)  # percentage of spend estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest Regression\n",
    "Random forest regression uses decision tree logic and may be helpful in predicting average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest of winsorized data\n",
    "#def random_forest_regression():\n",
    "rfr_df = wins_df.dropna().copy()\n",
    "\n",
    "x = rfr_df.GDP.values.reshape(-1, 1)\n",
    "y = rfr_df['Pct_Expenditure'].values\n",
    "regr = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regr.fit(x, y)  # The ideal fit line\n",
    "\n",
    "print(\"Random Forest Prediction: \", regr.predict([[1000]]))\n",
    "print(\"\\n\")\n",
    "\n",
    "x_array = np.arange(min(x), max(x), 0.01).reshape(-1, 1)  # line information to be drawn as a predict\n",
    "y_head = regr.predict(x_array)  # percentage of spend predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression of winsorized data\n",
    "def logistic_regression():\n",
    "    logi_df = wins_df.dropna().copy()  # Copy winsorized dataframe\n",
    "\n",
    "    logi_df.drop([\"Country\"], axis=1, inplace=True)  # Drop categorical feature\n",
    "    logi_df['Status'].value_counts()\n",
    "\n",
    "    plt.title('Logistic Regression', fontweight='bold')\n",
    "    plt.ylabel('Number of Rows')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.title(\"Logistic Regression\", fontweight='bold', fontsize=13)\n",
    "    plt.savefig('charts/Logistic_Regression.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "                format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "    logi_df.Status.value_counts().plot(kind='bar')\n",
    "    logi_df.Status = [1 if each == \"Developing\" else 0 for each in logi_df.Status]  # Convert to nominal vals\n",
    "    print(\"log_reg - logi_df.describe().T\", logi_df.describe().T)\n",
    "\n",
    "    # Normalize the data\n",
    "    y = logi_df['Status']\n",
    "    X_data = logi_df.drop(['Status'], axis=1)\n",
    "    X = (X_data - np.min(X_data)) / (np.max(X_data) - np.min(X_data)).values\n",
    "    \n",
    "    # Implement model\n",
    "    logi = sm.Logit(y, X_data)\n",
    "    logi_model = logi.fit()\n",
    "    print(\"Model Summary()\", logi_model.summary())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Create & plot summary\n",
    "    fig_size = (10, 8)  # w x h\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Logistical Model Summary\", fontsize=12, verticalalignment='baseline', horizontalalignment='center',\n",
    "              color='white', backgroundcolor='orange')\n",
    "    plt.text(0, 0, str(logi_model.summary()), fontsize=10, fontproperties='monospace', verticalalignment='bottom',\n",
    "             horizontalalignment='left')\n",
    "    plt.savefig(\"charts/Logistical_Model_Summary.png\", dpi=None, facecolor='w', edgecolor='g', orientation='portrait',\n",
    "                format=None, transparent=False, bbox_inches=None, pad_inches=0.05, metadata=None)\n",
    "\n",
    "    logi = LogisticRegression(solver=\"liblinear\")\n",
    "    logi_model = logi.fit(X, y)  # The ideal fit line\n",
    "\n",
    "    print(\"logi_model.intercept_ :\", logi_model.intercept_)  # TODO what is this?\n",
    "    print(\"logi_model.coef_ :\", logi_model.coef_)  # TODO what is this?\n",
    "\n",
    "    prediction_tune_model(logi_model, X, y)  # Call model tuning function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-TEST\n",
    "We can see that developed countries have a higher mean Life Expectancy.  We can use a t-test comparison to see if the difference is significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ttest to determine significance of difference between developed and developing\n",
    "#def ttest():\n",
    "ttest_results = stats.ttest_ind(wins_df.loc[wins_df['Status']=='Developed','Life_Expectancy'], wins_df.loc[wins_df['Status']=='Developing','Life_Expectancy'])\n",
    "\n",
    "print(\"ttest_results: \", ttest_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test results show that there is a significant difference between developing and developed countries when it comes to life expectancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prediction_tune_model(logi_model, X, y):\n",
    "y_pred = logi_model.predict(X)\n",
    "confusion_matrix(y, y_pred)\n",
    "accuracy_score(y, y_pred)\n",
    "\n",
    "# logi_model.predict(X)[0:10] # TODO do anything?\n",
    "# print(\"logi_model.predict_proba(X)\", logi_model.predict_proba(X)[0:10][:, 0:2])  # Set Top 10 TODO do anything?\n",
    "\n",
    "y_probs = logi_model.predict_proba(X)\n",
    "y_probs = y_probs[:, 1]\n",
    "# y_probs[0:10])  # Set top 10  TODO do anything?\n",
    "y_pred = [1 if i > 0.5 else 0 for i in y_probs]\n",
    "# y_pred[0:10])   TODO do anything?\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y, y_pred))  # Evaluate the accuracy of classification\n",
    "print(\"\\nAccuracy Score: \", accuracy_score(y, y_pred))  # Accuracy score\n",
    "print(\"\\n Classification Report:\\n\", classification_report(y, y_pred))  # Show main classification metrics\n",
    "\n",
    "logi_model.predict_proba(X)[:, 1][0:5] # TODO do anything?\n",
    "logit_roc_auc = roc_auc_score(y, logi_model.predict(X))\n",
    "fpr, tpr, thresholds = roc_curve(y, logi_model.predict_proba(X)[:, 1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "plt.title(\"ROC Curve\", fontweight='bold', fontsize=13)\n",
    "plt.savefig('charts/ROC_Curve.png', dpi=None, facecolor='w', edgecolor='g', orientation='landscape',\n",
    "            format=None, transparent=False, bbox_inches=None, pad_inches=0.10, metadata=None)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix allows us to see the high number of True Negatives (169) and True Positives (1362), greatly outnumbering the False Positives (73) and False Negatives (45).\n",
    "\n",
    "The model has proved to be 92.8% accurate.  I’d say that’s a success!   \n",
    "\n",
    "An f1-score of .96 is also impressive as the closer it is to 1, the better the precision and recall for the data. \n",
    "We can also see that the cross value score (uses k-fold) tells us that with this data the model scored 93.3%!  What’s nice here is the consistency in prediction accuracy scoring.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUMMARY\n",
    "The dataset began with 21 variables that were processed down to 12 independent variables (features) that describe Life Expectancy, the dependent variable.  \n",
    "\n",
    "The data was cleaned by first cleaning up the headings, then detecting and dealt with missing values, both inexplicit and explicit.  The data was also imputed and winsorized to address various missing values and outliers. From this a model was born. \n",
    "\n",
    "A number of machine learning methods were applied to the model including Linear Regression, Multiple Regression, Decision Tree regression, Random Forest Regression, and t-test.  \n",
    "\n",
    "Accuracy metrics like Confusion Matrix, Accuracy Score, Classification Report, and Cross Value Score were run, and the model scored an excellent 93% accuracy.\n",
    "\n",
    "While this project was more focused on predictions and the exploratory data process, future projects would benefit from what was learned here.  \n",
    "\n",
    "Additional questions to be answered in future projects:\n",
    "<ul>\n",
    "    <li>What is the impact of disease in developed versus developing nations?</li>\n",
    "<li>What are the impacts of vaccines on Life Expectancy in developing nations?</li>\n",
    "<li>Which immunizations have the highest correlation to greater life expectancy?</li>\n",
    "<li>What 5 features would be most impactful on life expectancy in developing countries?</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
